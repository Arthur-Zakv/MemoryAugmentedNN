{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a85d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306ea809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_one_hot(batch_cat_id, num_cats):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    batch_cat_id : torch.tensor [bs, seq_len, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    batch_cat_OH : torch.tensor [bs, seq_len, num_cats]\n",
    "    \n",
    "    \"\"\"\n",
    "    cat_samples = batch_cat_id.chunk(len(batch_cat_id), dim = 0)\n",
    "    batch_cat_OH = list()\n",
    "    for cat_sample in cat_samples:\n",
    "        cat_id = cat_sample.squeeze()\n",
    "        cat_OH = torch.zeros(len(cat_id), num_cats)\n",
    "        cat_OH[torch.arange(len(cat_id)), cat_id] = 1\n",
    "        batch_cat_OH.append(cat_OH)\n",
    "\n",
    "    return torch.stack(batch_cat_OH, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9ad636ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CESequenceLoss(p_y_x, y):\n",
    "    \"\"\"Cross Entropy Loss for Sequential Data\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    p_y_x : torch.tensor [bs, t, d]\n",
    "        Probability vector for all categories with values [0, 1]\n",
    "        \n",
    "    y : torch.tensor [bs, t, d]\n",
    "        One Hot Encoded ground truth labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    E : torch.tensor []\n",
    "    \"\"\"\n",
    "    log_p_y_x = nn.LogSoftmax(-1)(log_p_y_x)\n",
    "    log_p_y_x = torch.log(p_y_x).clamp(min = -100)\n",
    "    E_i_t = - (y * log_p_y_x).sum(dim = 2)\n",
    "    E_i = E_i_t.sum(dim = 1)\n",
    "    E = E_i.mean(dim = 0)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2fd1df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Controller : Recurrent Net Cell\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        cell : nn.GRUCell\n",
    "        output_embedder : nn.Linear\n",
    "        \n",
    "        Methods\n",
    "        -------\n",
    "        next_state(x_t, h, r) -> (h)\n",
    "        output(h, r) -> (y)\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        self.cell = nn.GRUCell(2 * d_hidden, d_hidden)\n",
    "        self.output_embedder = nn.Sequential(\n",
    "            nn.Linear(2 * d_hidden, d_hidden),\n",
    "            nn.Softmax(-1))\n",
    "            \n",
    "        \n",
    "    def next_state(self, x_t, h, r):\n",
    "        \"\"\"Concat input und read, return hidden state\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x_t : torch.tensor [bs, d_hidden]\n",
    "            Input timesteps t of sequence\n",
    "        \n",
    "        h : torch.tensor [bs, d_hidden]\n",
    "            Hidden State\n",
    "            \n",
    "        r : torch.tensor [bs, d_hidden]\n",
    "            Read \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        h : torch.tensor [bs, d_hidden]\n",
    "            Hidden State  \n",
    "        \"\"\"\n",
    "        \n",
    "        xr = torch.cat([x_t, r], dim = -1)\n",
    "        h = self.cell(xr, h)\n",
    "        return h\n",
    "        \n",
    "    def output(self, h, r):\n",
    "        \"\"\"Concat input und read, return hidden state\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h : torch.tensor [bs, d_hidden]\n",
    "            Hidden State\n",
    "            \n",
    "        r : torch.tensor [bs, d_hidden]\n",
    "            Read \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y : torch.tensor [bs, d_output]\n",
    "            Output, passed trough Activation\n",
    "        \"\"\"\n",
    "        \n",
    "        hr = torch.cat([h,r], dim = -1)\n",
    "        y = self.output_embedder(hr)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "199a4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralTuringMachine(nn.Module):\n",
    "    \"\"\"NTM : Total Model Infrastucture\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    controller\n",
    "    writing_head\n",
    "    reading_head\n",
    "    memory\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    initialize_state_and_read(bs) -> (h, r)\n",
    "    forward(x) -> (output)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralTuringMachine, self).__init__()\n",
    "        \n",
    "        self.controller = Controller()\n",
    "        self.writing_head = WritingHead()\n",
    "        self.reading_head = ReadingHead()\n",
    "        self.memory = Memory()\n",
    "        \n",
    "    def initialize_state_and_read(self, bs):\n",
    "        \"\"\"Initialize h and r for first timestep\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        bs : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        h : torch.tensor [bs, d_hidden]\n",
    "            initial hidden state\n",
    "            \n",
    "        r : torch.tensor [bs, d_hidden]\n",
    "            initial read\n",
    "        \"\"\"\n",
    "        h_0 = torch.zeros(x.shape[0], d_hidden)\n",
    "        r_0 = torch.zeros(x.shape[0], d_hidden)\n",
    "        return h_0, r_0\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \"\"\"Read in Sequence of length ht, output Sequence of length ft\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        ht : int\n",
    "            History sequence timesteps\n",
    "            \n",
    "        ft : int\n",
    "            Future sequence timesteps\n",
    "            \n",
    "        x : torch.tensor [bs, ht, d_input]\n",
    "            Input sequence\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output : torch.tensor [bs, ft, d_output]\n",
    "            Output  sequence\n",
    "        \"\"\"\n",
    "        bs = x.shape[0]\n",
    "        r,h = self.initialize_state_and_read(bs)\n",
    "        self.memory.init_w_previous(bs)\n",
    "        self.memory.init_memory(bs)\n",
    "        \n",
    "        output = list()\n",
    "        \n",
    "        for t in range(ft):\n",
    "            h = self.controller.next_state(x[:,t,:], h, r)\n",
    "            \n",
    "            k_r, ß_r, g_r, s_r, y_r = self.reading_head(h)\n",
    "            k_w, ß_w, g_w, s_w, y_w = self.writing_head(h)\n",
    "            e,a = self.writing_head.variables_for_memory(h)\n",
    "            \n",
    "            attention_r = self.memory.attention(k_r, ß_r, g_r, s_r, y_r)\n",
    "            attention_w = self.memory.attention(k_w, ß_w, g_w, s_w, y_w)\n",
    "\n",
    "            r = self.memory.read(attention_r)\n",
    "            self.memory.write(attention_w, e, a)\n",
    "            \n",
    "            y = self.controller.output(h,r)\n",
    "            output.append(y)\n",
    "            \n",
    "        output = torch.stack(output, dim = 1)    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9df2ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Head : Superclass of ReadingHead and WritingHead\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    project_to_key : nn.Linear\n",
    "    project_to_temperature : nn.Linear\n",
    "    project_to_gate : nn.Linear\n",
    "    project_to_shift : nn.Linear\n",
    "    project_to_gamma : nn.Linear\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    project_to_variables(h) -> (k, ß, g, s, y)\n",
    "    adjust_variables_for_attention(k, ß, g, s, y) -> (k, ß, g, s, y)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Head, self).__init__()\n",
    "        \n",
    "        self.project_to_key = nn.Linear(d_hidden, d_hidden)\n",
    "        self.project_to_temperature = nn.Linear(d_hidden, 1)\n",
    "        self.project_to_gate = nn.Linear(d_hidden, 1)\n",
    "        self.project_to_shift = nn.Linear(d_hidden, 3)\n",
    "        self.project_to_gamma = nn.Linear(d_hidden, 1)\n",
    "        \n",
    "         \n",
    "    def project_to_variables(self, h):\n",
    "        \"\"\"Create Parameters for Attention\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        h : torch.tensor [bs, d_hidden]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        k : torch.tensor [bs, d_hidden]\n",
    "        ß : torch.tensor [bs, 1]\n",
    "        g : torch.tensor [bs, 1]\n",
    "        s : torch.tensor [bs, 3]\n",
    "        y : torch.tensor [bs, 1]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        k = self.project_to_key(h)\n",
    "        ß = self.project_to_temperature(h)\n",
    "        g = self.project_to_gate(h)\n",
    "        s = self.project_to_shift(h)\n",
    "        y = self.project_to_gamma(h)\n",
    "        \n",
    "        return k, ß, g, s, y\n",
    "    \n",
    "    def adjust_variables_for_attention(self, k, ß, g, s, y):\n",
    "        \"\"\"Adjust variables to correct value ranges\n",
    "\n",
    "        Arguments\n",
    "        -------\n",
    "        k : torch.tensor [bs, d_hidden]\n",
    "        ß : torch.tensor [bs, 1]\n",
    "        g : torch.tensor [bs, 1]\n",
    "        s : torch.tensor [bs, 3]\n",
    "        y : torch.tensor [bs, 1]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        k : torch.tensor [bs, d_hidden]\n",
    "        ß : torch.tensor [bs, 1]\n",
    "        g : torch.tensor [bs, 1]\n",
    "        s : torch.tensor [bs, 3]\n",
    "        y : torch.tensor [bs, 1]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        k = k.clone()\n",
    "        ß = nn.ReLU()(ß)\n",
    "        g = nn.Sigmoid()(g)\n",
    "        y = torch.ones(1) + nn.ReLU()(y) \n",
    "        s = nn.Softmax(dim = 1)(s)\n",
    "        return k, ß, g, s, y\n",
    "        \n",
    "    \n",
    "    def forward(self, h):\n",
    "        k, ß, g, s, y = self.project_to_variables(h)\n",
    "        k, ß, g, s, y = self.adjust_variables_for_attention(k, ß, g, s, y)\n",
    "        return k, ß, g, s, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "eabdd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadingHead(Head):\n",
    "    \"\"\"ReadingHead : Subclass of Head\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    project_to_key : nn.Linear\n",
    "    project_to_temperature : nn.Linear\n",
    "    project_to_gate : nn.Linear\n",
    "    project_to_shift : nn.Linear\n",
    "    project_to_gamma : nn.Linear\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    project_to_variables(h) -> (k, ß, g, s, y)\n",
    "    adjust_variables_for_attention(k, ß, g, s, y) -> (k, ß, g, s, y)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReadingHead, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d2c8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingHead(Head):\n",
    "    \"\"\"WritingHead : Subclass of Head\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    project_to_key : nn.Linear\n",
    "    project_to_temperature : nn.Linear\n",
    "    project_to_gate : nn.Linear\n",
    "    project_to_shift : nn.Linear\n",
    "    project_to_gamma : nn.Linear\n",
    "    \n",
    "    project_to_add : nn.Linear\n",
    "    project_to_erase : nn.Linear\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    project_to_variables(h) -> (k, ß, g, s, y)\n",
    "    adjust_variables_for_attention(k, ß, g, s, y) -> (k, ß, g, s, y)\n",
    "    variables_for_memory(h) -> (e,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(WritingHead, self).__init__()\n",
    "        \n",
    "        self.project_to_add = nn.Linear(d_hidden, d_hidden)\n",
    "        self.project_to_erase = nn.Linear(d_hidden, d_hidden)\n",
    "        \n",
    "    def variables_for_memory(self, h):\n",
    "        \"\"\"Erase and Add for Memory writing\n",
    "        Arguments\n",
    "        ---------\n",
    "        h : torch.tensor [bs, d_hidden]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : torch.tensor [bs, d_hidden]\n",
    "        e : torch.tensor [bs, d_hidden]\n",
    "        \"\"\"\n",
    "        a = self.project_to_add(h)\n",
    "        e = self.project_to_erase(h)\n",
    "        return e,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "936369c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    N : int []\n",
    "        Number of Memory states\n",
    "        \n",
    "    M : torch.tensor [bs, N, d_hidden]\n",
    "        Memory Tensor\n",
    "        \n",
    "    w_previous : torch.tensor [bs, N]\n",
    "        Attention weights of the previous timestep\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    attention(self, k, ß, g, s, y) -> (w)        \n",
    "    read(w) -> (r)\n",
    "    write(w,e,a) -> void\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Memory, self).__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        \n",
    "        self.w_previous = nn.Softmax(-1)(torch.randn((bs, N)))\n",
    "        \n",
    "        self.init_w_previous_buffer()\n",
    "        self.init_memory_buffer()\n",
    "        \n",
    "    def init_w_previous_buffer(self):\n",
    "        \"\"\"Create Attention buffer\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        N : int\n",
    "            Number of Memory states\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        w_previous_buffer : torch.tensor [N]\n",
    "        \n",
    "        \"\"\"\n",
    "        self.register_buffer(\"w_previous_buffer\", nn.Softmax(-1)(torch.Tensor(self.N)))\n",
    "        \n",
    "    def init_w_previous(self, bs):\n",
    "        \"\"\"Create Attention buffer\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        bs : int\n",
    "        w_previous_buffer : torch.tensor [N]\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        w_previous : torch.tensor [bs, N]\n",
    "        \n",
    "        \"\"\"\n",
    "        self.w_previous = self.w_previous_buffer.clone().repeat(bs, 1)\n",
    "        \n",
    "    def init_memory_buffer(self):\n",
    "        \"\"\"Create Memory buffer\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        N : int\n",
    "            Number of Memory states\n",
    "            \n",
    "        d_hidden : int\n",
    "            Dimension of Memory states\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        M_init : torch.tensor [N, d_hidden]\n",
    "        \n",
    "        \"\"\"\n",
    "        self.register_buffer('M_init', torch.Tensor(N, d_hidden))\n",
    "        nn.init.uniform_(self.M_init, 0, 1)\n",
    "        \n",
    "    def init_memory(self, bs):\n",
    "        \"\"\"Expand Memory Buffer to batchsize for first timestep in Training\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        bs : int\n",
    "        M_init : torch.tensor [N, d_hidden]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        M : torch.tensor [bs, N, d_hidden]\n",
    "        \"\"\"\n",
    "        self.M = self.M_init.clone().repeat(bs, 1, 1)\n",
    "    \n",
    "    def attention_content_focus(self, k, ß):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        k : torch.tensor [bs, d_hidden]\n",
    "            Key, (technically it acts more as the query in this case)\n",
    "\n",
    "        M : torch.tensor [bs, N, d_hidden]\n",
    "            Memory, (technically it is the keys and values)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        w : torch.tensor [bs, N]\n",
    "\n",
    "\n",
    "                       < k * M[i] >\n",
    "        w_i = Softmax(-------------- * ß ) \n",
    "                       |k| * |M[i]|\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        dot_product = torch.bmm(self.M, k.unsqueeze(2)).squeeze(-1)\n",
    "        M_norm = torch.linalg.norm(self.M, dim = -1)\n",
    "        k_norm = torch.linalg.norm(k, dim = - 1).unsqueeze(-1)\n",
    "        mul_norms = M_norm * k_norm\n",
    "\n",
    "        alignment = dot_product / mul_norms\n",
    "        w = nn.Softmax(dim = 1)(alignment * ß)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def attention_location_focus(self, w_current, g):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        w : torch.tensor [bs, N]\n",
    "        w_previous : torch.tensor [bs, N]\n",
    "        g : torch.tensor [bs, 1]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        w_g : torch.tensor [bs, N]\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        w_g = g * w_current + (torch.tensor(1.) - g) * self.w_previous\n",
    "        return w_g\n",
    "\n",
    "    def attention_convolution(self, w, s):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        w : torch.tensor [bs, N]\n",
    "        s : torch.tensor [bs, 3]\n",
    "            the indices of s are [-1, 0, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        w_shifted : torch.tensor [bs, N]\n",
    "\n",
    "        \"\"\"\n",
    "        w_d = w[:,-1].unsqueeze(1)\n",
    "        w_0 = w[:,0].unsqueeze(1)\n",
    "        w_cycle = torch.cat([w_d, w, w_0], dim = -1).unsqueeze(1) # [bs, 1, N+2]\n",
    "        s = s.flip(dims = (1,)).unsqueeze(2) # [bs, 3, 1]\n",
    "\n",
    "        max_first_idx = w_cycle.shape[2] - 3\n",
    "        w_shifted = torch.cat([torch.bmm(w_cycle[:,:,i:i+3],s) \n",
    "                               for i in range(max_first_idx + 1)], \n",
    "                              dim = -1).squeeze(1)\n",
    "\n",
    "        return w_shifted\n",
    "\n",
    "\n",
    "    def attention_sharpen(self, w, y):\n",
    "        \"\"\"Apply Temperature to shifted weights\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        w : torch.tensor [bs, N]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        w : torch.tensor [bs, N]\n",
    "    \n",
    "        \"\"\"\n",
    "        nominator = (w ** y)\n",
    "        denominator = nominator.sum(dim = 1).unsqueeze(1)\n",
    "        \n",
    "        w = nominator / denominator\n",
    "        return w\n",
    "\n",
    "    def attention(self, k, ß, g, s, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        k : torch.tensor [bs, d_hidden]\n",
    "            Key, (technically it acts more as the query in this case)\n",
    "\n",
    "        M : torch.tensor [N, d_hidden]\n",
    "            Memory, (technically it is the keys and values)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        w : torch.tensor [bs, N]\n",
    "\n",
    "        \"\"\"\n",
    "        w = self.attention_content_focus(k,ß)\n",
    "        w = self.attention_location_focus(w, g)\n",
    "        w = self.attention_convolution(w, s)\n",
    "        w = self.attention_sharpen(w, y)\n",
    "        \n",
    "        self.w_previous = w\n",
    "        return w\n",
    "    \n",
    "    def read(self, w):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        w : torch.tensor [bs, N]\n",
    "\n",
    "        M : torch.tensor [bs, N, d_hidden]\n",
    "            Memory, (technically it is the keys and values)\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        r : torch.tensor [bs, d_hidden]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        r = torch.bmm(w.unsqueeze(1), self.M).squeeze(1)\n",
    "        return r\n",
    "    \n",
    "    \n",
    "    def write(self, w, e, a):\n",
    "        \"\"\"Create Memory Matrix for next timestep\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        M : torch.tensor [bs, N, d_hidden]\n",
    "        w : torch.tensor [bs, N]\n",
    "        e : torch.tensor [bs, d_hidden]\n",
    "        a : torch.tensor [bs, d_hidden]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        M : torch.tensor [bs, N, d_hidden]\n",
    "\n",
    "            Mt[i] = Mt-1[i] * (I - w[i] * diag(e)) + w[i] * a\n",
    "\n",
    "        \"\"\"\n",
    "        bs = w.shape[0]\n",
    "        M_next = list()\n",
    "        \n",
    "        for i in range(N):\n",
    "            w_i = w[:,i].reshape(bs, 1, 1)         # [bs, 1, 1]\n",
    "            I = torch.eye(d_hidden).repeat(bs,1,1) # [bs, d, d]\n",
    "            e_diag = torch.diag_embed(e)           # [bs, d, d]\n",
    "            M_i = self.M[:,i,:].unsqueeze(1)       # [bs, 1, d]\n",
    "            \n",
    "            M_i = torch.bmm(M_i, I - w_i * e_diag) + w_i * a.unsqueeze(1) \n",
    "            M_next.append(M_i)\n",
    "            \n",
    "        M_next = torch.cat(M_next, dim = 1)\n",
    "        self.M = M_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2d405db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "bs = 256\n",
    "ht = 6\n",
    "ft = 6\n",
    "num_cats = 5\n",
    "d_hidden = num_cats\n",
    "N = 4\n",
    "\n",
    "# Data\n",
    "x = torch.randint(0,num_cats-1, (bs, ht, 1))\n",
    "x_OH = batch_to_one_hot(x, num_cats)\n",
    "\n",
    "model = NeuralTuringMachine()\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ffc7e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 10.45910358428955\n",
      "loss = 9.353789329528809\n",
      "loss = 8.495030403137207\n",
      "loss = 7.749838829040527\n",
      "loss = 6.872945308685303\n",
      "loss = 5.709312915802002\n",
      "loss = 4.470155239105225\n",
      "loss = 2.952481508255005\n",
      "loss = 1.949126124382019\n",
      "loss = 1.43279230594635\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    y_pred = model(x_OH)\n",
    "    loss = CESequenceLoss(y_pred, x_OH)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'loss = {loss.detach()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
